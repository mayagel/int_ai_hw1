{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AHjIAUOGMLiI"
   },
   "source": [
    "# Welcome to Intro to Artificial Inteligence assignment 1 practical part!\n",
    "\n",
    "In this part you will get to implement algotithms learned in class from scratch.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pupose of this notebook is to provide the requirments of the practical part and some visualizations. Your implementation should be in the provided Algorithms.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUhq1ubdDE2v"
   },
   "source": [
    "**Important tip:** If the same variable name appears in two different cells, the variable value will be determined by the last cell to run, rather than by the position of the cell. Let's see an example,  run the three cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kei_W6-QEQD_"
   },
   "outputs": [],
   "source": [
    "# cell 2\n",
    "x = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tO3WdjItEVQh"
   },
   "source": [
    "now rerun cell 1 and print x again.\n",
    "\n",
    "The same applies to functions, classes, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XIvTBylGni-J"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from DragonBallEnv import DragonBallEnv\n",
    "from typing import List, Tuple\n",
    "from Algorithms import *\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EWEiS5mVn7mg"
   },
   "source": [
    "# Maps\n",
    "A map can be produced manually as shown in the cell below. We will only work on maps in which there is a route from the initial state to a final state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIpo4KAgxE9q"
   },
   "source": [
    "\n",
    "# The Dragon Ball Environment â›·\n",
    "The file `DragonBallEnv.py` implements our own version of the dragon ball environment. It is recommended to go through the code. \n",
    "\n",
    "**Note: You are not allowed to change this file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9ce62lyU8L39"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAPS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m DragonBallEnv(\u001b[43mMAPS\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m8x8\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitial state:\u001b[39m\u001b[38;5;124m'\u001b[39m, state)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MAPS' is not defined"
     ]
    }
   ],
   "source": [
    "env = DragonBallEnv(MAPS[\"8x8\"])\n",
    "state = env.reset()\n",
    "print('Initial state:', state)\n",
    "print('Goal states:', env.goals) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cg7XAAByJ-IH"
   },
   "outputs": [],
   "source": [
    "print(f\"Action Space {env.action_space}\")\n",
    "print(f\"State Space {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BASrtg6SHjFI"
   },
   "source": [
    "Now we will go through some usfel methods (It is still recommended to go through the other methods in the class):\n",
    "\n",
    "`render()` - returns a printable view of the board."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAaEmhts9vAT"
   },
   "source": [
    "The pink square represents the agent. The letter ×´*H*×´ represents holes, and the yellow square are dragon balls or goal.\n",
    "\n",
    "Here are two more useful methods:\n",
    "\n",
    "`get_state()` - Returns the current state of the agent.\n",
    "\n",
    "`set_state(state)` - Sets the current state of the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VwmIbSyAcsC"
   },
   "source": [
    "`succ(state)` - Returns a dictionary that contains information on all the successors of a state.\n",
    "\n",
    "*   The keys are the actions.\n",
    "*   The values are tuples of the form (next state, cost, terminated). Note that terminated is true when the agent reaches a **final state** or a **hole**.\n",
    "\n",
    "\n",
    "\n",
    "***Tip***: You can loop through both keys and values by using the `items()` method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2qFBBDWBcVi"
   },
   "source": [
    "As you can see, the action 0 (down) will move your agent to state 26 and the transition will cost you 10. Action 1 will move your agent to state 19, which is a hole that will terminate your run.\n",
    "\n",
    "`is_final_state(state)` can assist you in distinguishing between a final state and a hole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptr9kbNole-Y"
   },
   "source": [
    "Let's see what happens when we apply succ(state) on a hole:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxny4BVOr-UT"
   },
   "source": [
    "As you can see, if the operator cannot be applied to the state, all returned values are \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLWj_oYaFM33"
   },
   "outputs": [],
   "source": [
    "new_state, cost, terminated = env.step(DOWN)\n",
    "print(env.render())\n",
    "print(\"New state:\", new_state)\n",
    "print(\"cost:\", cost)\n",
    "print(\"Terminated:\", terminated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoxs3sG0QyhF"
   },
   "source": [
    "We've finished our demo ðŸ¥³ and it's time to reset the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfjMsGRnHoK0"
   },
   "outputs": [],
   "source": [
    "print(f\"current state befor reset: {env.get_state()}\")\n",
    "env.reset()\n",
    "print(f\"current state after reset: {env.get_state()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZE__OgBx78N"
   },
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "  def __init__(self):\n",
    "    self.env = None\n",
    "\n",
    "  def animation(self, epochs: int ,state: int, action: List[int], total_cost: int) -> None:\n",
    "      clear_output(wait=True)\n",
    "      print(self.env.render())\n",
    "      print(f\"Timestep: {epochs}\")\n",
    "      print(f\"State: {state}\")\n",
    "      print(f\"Action: {action}\")\n",
    "      print(f\"Total Cost: {total_cost}\")\n",
    "      time.sleep(1)\n",
    "\n",
    "  def random_search(self, DragonBallEnv: env) -> Tuple[List[int],int]:\n",
    "    self.env = env\n",
    "    self.env.reset()\n",
    "    epochs = 0\n",
    "    cost = 0\n",
    "    total_cost = 0\n",
    "\n",
    "    actions = []\n",
    "\n",
    "    state = self.env.get_initial_state()\n",
    "    while not self.env.is_final_state(state):\n",
    "      action = self.env.action_space.sample()\n",
    "      new_state, cost, terminated = self.env.step(action)\n",
    "        \n",
    "      while terminated is True and self.env.is_final_state(state) is False:\n",
    "        self.env.set_state(state)\n",
    "        action = self.env.action_space.sample()\n",
    "        new_state, cost, terminated = self.env.step(action)\n",
    "        \n",
    "      actions.append(action)\n",
    "      total_cost += cost\n",
    "      state = new_state\n",
    "      epochs += 1\n",
    "      \n",
    "      self.animation(epochs,state,action,total_cost)\n",
    "\n",
    "    return (actions, total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Gc3-gJVZH3h"
   },
   "outputs": [],
   "source": [
    "# agent = RandomAgent()\n",
    "# agent.random_search(env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qBKDB1Aja5JB"
   },
   "source": [
    "As you can see, a random policy is, unsurprisingly, not a good policy. However, what else can we do?\n",
    "\n",
    "This is where you come in!\n",
    "\n",
    "In this assignment you will be required to implement the following algorithms taught in class in order to solve the problem.\n",
    "\n",
    "Algorithms: \n",
    "1. BFS-G\n",
    "2. W-A*\n",
    "3. epsilon-A*\n",
    "\n",
    "Important to note!\n",
    "\n",
    "Each agent should return a tuple: (actions, cost, expended) \n",
    "*  actions - the list of integers containing the sequence of actions that produce your agent's solution (and not the entire search process).\n",
    "* cost -  an integer which holds the total cost of the solution.\n",
    "* expanded - an integer which holds the number of nodes that have been expanded during the search (A node is considered expanded if we check for it's successors).\n",
    "\n",
    "The solution to our search problem is the a to the final state, not the final state itself. By saving the actions, we are able to restore the path your agent found.\n",
    "\n",
    "\n",
    "Any other output, unless otherwise specified, will cause the running of the tests to fail and will result in a grade of 0 !\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQj77NFT0Wdc"
   },
   "outputs": [],
   "source": [
    "def print_solution(actions,env: DragonBallEnv) -> None:\n",
    "    env.reset()\n",
    "    total_cost = 0\n",
    "    print(env.render())\n",
    "    print(f\"Timestep: {1}\")\n",
    "    print(f\"State: {env.get_state()}\")\n",
    "    print(f\"Action: {None}\")\n",
    "    print(f\"Cost: {0}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for i, action in enumerate(actions):\n",
    "      state, cost, terminated = env.step(action)\n",
    "      total_cost += cost\n",
    "      clear_output(wait=True)\n",
    "\n",
    "      print(env.render())\n",
    "      print(f\"Timestep: {i + 2}\")\n",
    "      print(f\"State: {state}\")\n",
    "      print(f\"Action: {action}\")\n",
    "      print(f\"Cost: {cost}\")\n",
    "      print(f\"Total cost: {total_cost}\")\n",
    "      \n",
    "      time.sleep(1)\n",
    "\n",
    "      if terminated is True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfoBu-elP2To"
   },
   "source": [
    "Now lets test your BFS agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, False, False)\n",
      "(8, False, False)\n",
      "(1, False, False)\n",
      "(16, False, False)\n",
      "(9, False, False)\n",
      "(2, False, False)\n",
      "(24, False, False)\n",
      "(17, False, False)\n",
      "(10, False, False)\n",
      "(3, False, False)\n",
      "(32, False, False)\n",
      "(25, False, False)\n",
      "(18, False, False)\n",
      "(11, False, False)\n",
      "(4, False, False)\n",
      "(40, False, False)\n",
      "(33, False, False)\n",
      "(26, False, False)\n",
      "(19, False, False)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     33\u001b[0m BFS_agent \u001b[38;5;241m=\u001b[39m BFSAgent()\n\u001b[0;32m---> 34\u001b[0m actions, total_cost, expanded \u001b[38;5;241m=\u001b[39m \u001b[43mBFS_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal_cost: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_cost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpanded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpanded\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/technion/intro AI/hw1/Algorithms.py:88\u001b[0m, in \u001b[0;36mBFSAgent.search\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     86\u001b[0m in_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m CLOSED:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[43mnew_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     89\u001b[0m         in_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "# all the imports and equals lines in one cell\n",
    "# import importlib\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from DragonBallEnv import DragonBallEnv\n",
    "from typing import List, Tuple\n",
    "from Algorithms import *\n",
    "# importlib.reload(BFSAgent)\n",
    "DOWN = 0\n",
    "RIGHT = 1\n",
    "UP = 2\n",
    "LEFT = 3\n",
    "MAPS = {\n",
    "    \"4x4\": [\"SFFF\",\n",
    "            \"FDFF\",\n",
    "            \"FFFD\",\n",
    "            \"FFFG\"],\n",
    "    \"8x8\": [\n",
    "        \"SFFFFFFF\",\n",
    "        \"FFFFFTAL\",\n",
    "        \"TFFHFFTF\",\n",
    "        \"FFFFFHTF\",\n",
    "        \"FAFHFFFF\",\n",
    "        \"FHHFFFHF\",\n",
    "        \"DFTFHDTL\",\n",
    "        \"FLFHFFFG\",\n",
    "    ],\n",
    "}\n",
    "env = DragonBallEnv(MAPS[\"8x8\"])\n",
    "state = env.reset()\n",
    "\n",
    "BFS_agent = BFSAgent()\n",
    "actions, total_cost, expanded = BFS_agent.search(env)\n",
    "print(f\"Total_cost: {total_cost}\")\n",
    "print(f\"Expanded: {expanded}\")\n",
    "print(f\"Actions: {actions}\")\n",
    "\n",
    "assert total_cost == 119.0, \"Error in total cost returned\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sUnh0cwqrzfK"
   },
   "source": [
    "## 2. Weighted A*\n",
    "TO DO: implement Wighted A* like shown in class.\n",
    "\n",
    "Note:\n",
    "\n",
    "*   A parameter called `h_weight` is passed to `Greedy_Best_First_search()`, which indicates how much weight is given to the heuristics (ranging from 0 to 1).\n",
    "*   The heurisitcs needed to be implemented. Instructions in dry pdf.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xKSoHrMvJTG"
   },
   "outputs": [],
   "source": [
    "WA_agent = WeightedAStarAgent()\n",
    "actions, total_cost, expanded = WA_agent.search(env, h_weight=0.5)\n",
    "print(f\"Total_cost: {total_cost}\")\n",
    "print(f\"Expanded: {expanded}\")\n",
    "print(f\"Actions: {actions}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A*-epsilon::\n",
    "TO DO: implement A*-epsilon: like shown in class.\n",
    "\n",
    "use the same heuristic as in previous sections.\n",
    "\n",
    "Note:\n",
    "*   A parameter called `epsilon` is passed to `A_star_epsilon_search()`.\n",
    "*   We will not test the amount of expanded nodes for this algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_solution(actions, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "test_boards = {\n",
    "\"map12x12\": \n",
    "['SFAFTFFTHHHF',\n",
    "'AFLTFFFFTALF',\n",
    "'LHHLLHHLFTHD',\n",
    "'HALTHAHHADHF',\n",
    "'FFFTFHFFAHFL',\n",
    "'LLTHFFFAHFAT',\n",
    "'HAAFFALHTATF',\n",
    "'LLLFHFFHTLFH',\n",
    "'FATAFHTTFFAF',\n",
    "'HHFLHALLFTLF',\n",
    "'FFAFFTTAFAAL',\n",
    "'TAAFFFHAFHFG'],\n",
    "\"map15x15\": \n",
    "['SFTTFFHHHHLFATF',\n",
    "'ALHTLHFTLLFTHHF',\n",
    "'FTTFHHHAHHFAHTF',\n",
    "'LFHTFTALTAAFLLH',\n",
    "'FTFFAFLFFLFHTFF',\n",
    "'LTAFTHFLHTHHLLA',\n",
    "'TFFFAHHFFAHHHFF',\n",
    "'TTFFLFHAHFFTLFD',\n",
    "'TFHLHTFFHAAHFHF',\n",
    "'HHAATLHFFLFFHLH',\n",
    "'FLFHHAALLHLHHAT',\n",
    "'TLHFFLTHFTTFTTF',\n",
    "'AFLTDAFTLHFHFFF',\n",
    "'FFTFHFLTAFLHTLA',\n",
    "'HTFATLTFHLFHFAG'],\n",
    "\"map20x20\" : \n",
    "['SFFLHFHTALHLFATAHTHT',\n",
    "'HFTTLLAHFTAFAAHHTLFH',\n",
    "'HHTFFFHAFFFFAFFTHHHT',\n",
    "'TTAFHTFHTHHLAHHAALLF',\n",
    "'HLALHFFTHAHHAFFLFHTF',\n",
    "'AFTAFTFLFTTTFTLLTHDF',\n",
    "'LFHFFAAHFLHAHHFHFALA',\n",
    "'AFTFFLTFLFTAFFLTFAHH',\n",
    "'HTTLFTHLTFAFFLAFHFTF',\n",
    "'LLALFHFAHFAALHFTFHTF',\n",
    "'LFFFAAFLFFFFHFLFFAFH',\n",
    "'THHTTFAFLATFATFTHLLL',\n",
    "'HHHAFFFATLLALFAHTHLL',\n",
    "'HLFFFFHFFLAAFTFFDAFH',\n",
    "'HTLFTHFFLTHLHHLHFTFH',\n",
    "'AFTTLHLFFLHTFFAHLAFT',\n",
    "'HAATLHFFFHHHHAFFFHLH',\n",
    "'FHFLLLFHLFFLFTFFHAFL',\n",
    "'LHTFLTLTFATFAFAFHAAF',\n",
    "'FTFFFFFLFTHFTFLTLHFG']}\n",
    "\n",
    "test_envs = {}\n",
    "for board_name, board in test_boards.items():\n",
    "    test_envs[board_name] = DragonBallEnv(board)\n",
    "\n",
    "\n",
    "BFS_agent = BFSAgent()\n",
    "WAStar_agent = WeightedAStarAgent()\n",
    "\n",
    "weights = [0.5, 0.7, 0.9]\n",
    "\n",
    "agents_search_function = [\n",
    "    BFS_agent.search,\n",
    "]\n",
    "\n",
    "header = ['map',  \"BFS-G cost\",  \"BFS-G expanded\",\\\n",
    "           'WA* (0.5) cost', 'WA* (0.5) expanded', 'WA* (0.7) cost', 'WA* (0.7) expanded', 'WA* (0.9) cost', 'WA* (0.9) expanded']\n",
    "\n",
    "with open(\"results.csv\", 'w') as f:\n",
    "  writer = csv.writer(f)\n",
    "  writer.writerow(header)\n",
    "  for env_name, env in test_envs.items():\n",
    "    data = [env_name]\n",
    "    for agent in agents_search_function:\n",
    "      _, total_cost, expanded = agent(env)\n",
    "      data += [total_cost, expanded]\n",
    "    for w in weights:\n",
    "        _, total_cost, expanded = WAStar_agent.search(env, w)\n",
    "        data += [total_cost, expanded]\n",
    "\n",
    "    writer.writerow(data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
